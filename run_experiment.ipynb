{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import time\n",
    "import warnings\n",
    "import random, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from scipy.stats import spearmanr\n",
    "from pprint import pprint\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d88261",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAPLAN_category = ['NAPLAN-Numeracy','NAPLAN-Reading', 'NAPLAN-Writting']\n",
    "def read_data():\n",
    "    train_data = pd.read_csv(\"../forML/train_2261.csv\")\n",
    "    test_data = pd.read_csv(\"../forML/test_2261.csv\")\n",
    "    target_train = train_data.loc[:,NAPLAN_category]\n",
    "    target_test = test_data.loc[:,NAPLAN_category]\n",
    "    x_train = train_data.drop(NAPLAN_category,1)\n",
    "    x_test = test_data.drop(NAPLAN_category,1)\n",
    "    y_train, y_test = dict(), dict()\n",
    "    y_train[\"numeracy\"], y_train[\"reading\"], y_train[\"writing\"] = target_train[['NAPLAN-Numeracy']],target_train[['NAPLAN-Reading']],target_train[['NAPLAN-Writting']]\n",
    "    y_test[\"numeracy\"], y_test[\"reading\"], y_test[\"writing\"] = target_test[['NAPLAN-Numeracy']],target_test[['NAPLAN-Reading']],target_test[['NAPLAN-Writting']]\n",
    "    return x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test = read_data()\n",
    "numeracy_train = y_train[\"numeracy\"]\n",
    "numeracy_test = y_test[\"numeracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb803ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "maes,rmses,r2s = [],[],[]\n",
    "preds = []\n",
    "for _ in range(10):\n",
    "    lr.fit(x_train, numeracy_train)\n",
    "    mae,rmse,r2,y_test_pred = evaluate(lr,x_test,numeracy_test)\n",
    "    evaluate_fairness(y_test_pred,numeracy_test,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9849bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 3)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4,8]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e358411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "for category in y_train:\n",
    "    rf_random.fit(x_train, y_train[category])\n",
    "    best_random = rf_random.best_estimator_\n",
    "    print(rf_random.best_params_, rf_random.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca915c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking numeracy test as an example, after we obtained the initial ranges for hyperparameters, \n",
    "# we mannually finr-tuned the paratermeters as follows\n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [25,32,40],\n",
    "    'max_features': ['sqrt'],\n",
    "    'min_samples_leaf': [1,2, 3],\n",
    "    'min_samples_split': [4,5,8],\n",
    "    'n_estimators': [500, 600, 700]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(x_train,numeracy_train)\n",
    "grid_search.best_params_, grid_search.best_score_\n",
    "#******************************************************Test*************************************************************************\n",
    "rf = RandomForestRegressor(max_depth=70,max_features='sqrt',min_samples_leaf=1, min_samples_split=3,n_estimators=1000,bootstrap=False,n_jobs=-1)\n",
    "maes,rmses,r2s = [],[],[]\n",
    "preds = []\n",
    "for _ in range(10): \n",
    "    rf.fit(x_train,numeracy_train)\n",
    "    mae,rmse,r2,y_test_pred = evaluate(rf,x_test,numeracy_test,print_info=False)\n",
    "    evaluate_fairness(y_test_pred,numeracy_test,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "x_train_sc = scaler.fit_transform(x_train)\n",
    "x_test_sc = scaler.transform(x_test)\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(500, 240, 120), (480, 240, 120), (500, 200, 100)],\n",
    "    'activation': ['identity'],\n",
    "    'solver': ['adam','lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'learning_rate':['constant']\n",
    "}\n",
    "mlp = MLPRegressor(random_state=42, max_iter=3000,early_stopping=True,n_iter_no_change=20)\n",
    "grid = GridSearchCV(mlp, param_grid, n_jobs= -1, cv=5)\n",
    "grid.fit(x_train_sc,numeracy_train)\n",
    "print(grid.best_params_) \n",
    "#******************************************************Test*************************************************************************\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(200,100),solver=\"lbfgs\", activation=\"identity\" ,alpha=0.0001,learning_rate='constant',random_state=42, max_iter=3000,early_stopping=True,n_iter_no_change=20)\n",
    "for _ in range(10): \n",
    "    mlp.fit(x_train_sc,numeracy_train.values)\n",
    "    mae,rmse,r2,y_test_pred = evaluate(mlp,x_test_sc,numeracy_test.values,print_info=False)\n",
    "    evaluate_fairness(y_test_pred,numeracy_test,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, X_train, y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=20):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='mae', early_stopping_rounds=early_stopping_rounds, verbose_eval=False)\n",
    "#         print(cvresult['test-rmse-mean'].min())\n",
    "#         print(cvresult['test-rmse-mean'].argmin())\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        #print(cvresult.shape)\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(X_train, y_train,eval_metric='mae')\n",
    "        \n",
    "    #Predict training set:\n",
    "    evaluate(alg,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,15,2),\n",
    " 'min_child_weight':range(1,14,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, scale_pos_weight=1, seed=42), \n",
    " param_grid = param_test1,scoring='neg_mean_absolute_error',n_jobs=4,cv=10)\n",
    "gsearch1.fit(x_train,numeracy_train)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test2 = {\n",
    " 'gamma':[i/100.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = xgb.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=8,\n",
    " min_child_weight=7, gamma=0, subsample=0.8, colsample_bytree=0.8,nthread=4, scale_pos_weight=1, seed=42), \n",
    " param_grid = param_test2,scoring='neg_mean_absolute_error',n_jobs=4,cv=5)\n",
    "gsearch2.fit(x_train,numeracy_train)\n",
    "gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test3 = {\n",
    " 'subsample':[i/10.0 for i in range(2,11)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(2,11)],\n",
    "  'n_estimators':[200, 400, 600, 800, 1000, 2000] \n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgb.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=8,\n",
    " min_child_weight=7, gamma=0.02, subsample=0.8, colsample_bytree=0.8,nthread=4, scale_pos_weight=1, seed=42), \n",
    " param_grid = param_test3,scoring='neg_mean_absolute_error',n_jobs=4,cv=5)\n",
    "gsearch3.fit(x_train,numeracy_train)\n",
    "gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test4 = {\n",
    "    'learning_rate': [0.001,0.01,0.1],\n",
    " 'reg_alpha':[1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 100],\n",
    " 'reg_lambda':[1e-5, 1e-4, 1e-3, 1e-2, 0.1, 1, 100]   \n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgb.XGBRegressor(learning_rate =0.1, n_estimators=1000, max_depth=5,\n",
    " min_child_weight=4, gamma=0.02, subsample=0.4, colsample_bytree=0.7, nthread=4, scale_pos_weight=1, seed=42), \n",
    " param_grid = param_test4,scoring='neg_mean_absolute_error',n_jobs=4,cv=5)\n",
    "gsearch4.fit(x_train,numeracy_train)\n",
    "gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best = xgb.XGBRegressor(\n",
    "                 gamma=0.02,                 \n",
    "                 learning_rate=0.1,\n",
    "                 max_depth=8,\n",
    "                 min_child_weight=6,\n",
    "                 n_estimators=1000,                                                                    \n",
    "                 colsample_bytree=0.7,\n",
    "                 subsample=0.4,\n",
    "                 reg_alpha=0.0001, \n",
    "                 reg_lambda=1,\n",
    "                 seed=42)\n",
    "for _ in range(10): \n",
    "    modelfit(xgb_best, x_train,numeracy_train)\n",
    "    mae,rmse,r2,y_test_pred = evaluate(xgb_best,x_test,numeracy_test,print_info=True)\n",
    "    evaluate_fairness(y_test_pred,numeracy_test,x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
