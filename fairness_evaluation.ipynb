{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "import random, json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7381fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring the overall fairness w.r.t., a demographic attribute (Section 4.2.1) using：\n",
    "# - OSA(Overall Score Accuracy)：\n",
    "# - OSD(Overall Score Difference)\n",
    "# - OSA(COnditional Score Difference)\n",
    "# Reference: Loukina, Anastassia, Nitin Madnani, and Klaus Zechner. \"The many dimensions of algorithmic fairness in educational applications.\" Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications. 2019.\n",
    "def overall_fairness(predicted,actual,demo,measure=\"OSA\"):\n",
    "    demo=demo.to_numpy()\n",
    "    if demo.ndim<2:\n",
    "        demo=np.reshape(demo,[-1,1])\n",
    "    lr = LinearRegression()    \n",
    "    if measure == \"OSA\":\n",
    "        error = np.square(np.array(predicted)-np.squeeze(np.array(actual)))\n",
    "        error = np.reshape(sq_error,[-1,1])\n",
    "        feature = demo\n",
    "        lr.fit(feature, error)\n",
    "        pred = lr.predict(demo)\n",
    "        fairness = metrics.r2_score(error, pred)\n",
    "    elif measure == \"OSD\":\n",
    "        error = np.array(predicted)-np.squeeze(np.array(actual))\n",
    "        error = np.reshape(sq_error,[-1,1])\n",
    "        feature = demo\n",
    "        lr.fit(feature, error)\n",
    "        pred = lr.predict(demo)\n",
    "        fairness = metrics.r2_score(error, pred)\n",
    "    elif measure = \"CSD\":\n",
    "        error = np.array(predicted)-np.squeeze(np.array(actual))\n",
    "        error = np.reshape(error,[-1,1])\n",
    "        feature = pd.concat([demo,actual],axis=1)\n",
    "        feature=feature.to_numpy()\n",
    "        if feature.ndim<2:\n",
    "            feature=np.reshape(feature,[-1,1])\n",
    "        lr.fit(actual, error)\n",
    "        pred1 = lr.predict(actual)\n",
    "        lr.fit(feature, error)\n",
    "        pred2 = lr.predict(feature)\n",
    "        fairness = metrics.r2_score(error, pred2) - metrics.r2_score(error, pred1)        \n",
    "    else:\n",
    "        print(\"Only OSA, OSD, and CSD are available currently!\")\n",
    "        return\n",
    "    \n",
    "    return fairness    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at fairness w.r.t., a specific group (Section 4.2.2) using: \n",
    "# - SMD(Standardized Mean Difference): checking the difference between standardized predicted scores and actual scores for each subgroup\n",
    "# - MAE(Mean Abolute Error): checking the mean absolute error of each subgroup\n",
    "# Reference: Loukina, Anastassia, Nitin Madnani, and Klaus Zechner. \"The many dimensions of algorithmic fairness in educational applications.\" Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications. 2019.\n",
    "def subgroup_fairness(predicted,actual,demo,measure=\"SMD\"):\n",
    "    groups = set(demo)\n",
    "    n_groups = len(groups)\n",
    "    index_by_group = {k:[] for k in groups}\n",
    "    for i in range(len(demo)):\n",
    "        v = demo[i]\n",
    "        index_by_group[v].append(i) \n",
    "        \n",
    "    predicted =np.array(predicted)\n",
    "    actual = np.array(actual)\n",
    "    if measure == \"SMD\":\n",
    "        zs_pred = np.squeeze(stats.zscore(predicted))\n",
    "        zs_actual = np.squeeze(stats.zscore(actual))\n",
    "    elif measure  == \"MAE\":\n",
    "        zs_pred = predicted\n",
    "        zs_actual = actual\n",
    "    else:\n",
    "        print(\"Only SMD and MAE are availiable currently!\")\n",
    "    \n",
    "    fairness_bygroup = {}\n",
    "    for g in groups:\n",
    "        g_ids = np.array(index_by_group[g])\n",
    "        preds_g = zs_pred[g_ids]\n",
    "        actuals_g = zs_actual[g_ids]\n",
    "        zs_diff = preds_g-actuals_g\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            if measure == \"SMD\":\n",
    "                metric = np.nanmean(zs_diff, axis=0)               \n",
    "            elif measure  == \"MAE\":\n",
    "                metric = metrics.mean_absolute_error(actuals_g, preds_g)\n",
    "            else:\n",
    "                return\n",
    "        print(\"%s for %s is %f with %d students.\"%(measure,g,smd,len(g_ids)))\n",
    "        fairness_bygroup[g] = metric\n",
    "    return fairness_bygroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d514a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the fairness w.r.t., Performance Gap using (Section 4.2.3)\n",
    "# - KS(Kolmogorov-Smirnov distance metric), which has been frequently used to measure the difference between two data distributions. \n",
    "# Reference: Chzhen, Evgenii, et al. \"Fair regression with wasserstein barycenters.\" Advances in Neural Information Processing Systems 33 (2020): 7321-7331.\n",
    "def getKS(data_col,demo):\n",
    "    data_col =np.array(data_col)\n",
    "    min_y =min(data_col)\n",
    "    max_y = max(data_col)\n",
    "    tt=np.linspace(min(data_col),max(data_col),1000)\n",
    "    v = np.array(demo)\n",
    "    ids_v1 = np.where(v==1)[0]\n",
    "    ids_v0 = np.where(v==0)[0]\n",
    "    n_g = (len(ids_v0),len(ids_v1))\n",
    "    if 0 in n_g:\n",
    "        ks = np.inf\n",
    "    else:\n",
    "        ks = 0\n",
    "        for t in tt:\n",
    "            ks = max(ks, abs(sum(data_col[ids_v0]<=t)/n_g[0]-sum(data_col[ids_v1]<=t)/n_g[1]))\n",
    "    return ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_attrs = ['Gender','Disability','NCCD-Funded','Kinder_Age', 'NumAbvYear9','NumAbvDiploma','NumProf','NumSibling','SiblingOrder']\n",
    "def evaluate_fairness(y_pred,y_test,feature_test):\n",
    "    aggregated_fairness = {}\n",
    "    for demo_attr in demo_attrs:\n",
    "        demo_col = feature_test[demo_attr]\n",
    "        aggregated_fairness[\"OSA\"] = overall_fairness(y_pred, y_test, demo_col, measure=\"OSA\")\n",
    "        aggregated_fairness[\"OSD\"] = overall_fairness(y_pred, y_test, demo_col, measure=\"OSD\")\n",
    "        aggregated_fairness[\"CSD\"] = overall_fairness(y_pred, y_test, demo_col, measure=\"CSD\")\n",
    "     \n",
    "        fairness_by_group = {}\n",
    "        fairness_by_group[\"SMD\"] = subgroup_fairness(y_pred, y_test, demo_col, measure=\"SMD\")\n",
    "        fairness_by_group[\"MAE\"] = subgroup_fairness(y_pred, y_test, demo_col, measure=\"MAE\")\n",
    "\n",
    "        KS = getKS(y_pred,demo_col)\n",
    "        print(\"For attribute \", demo_attr)\n",
    "        Print(\"aggregated fairness: \", aggregated_fairness)\n",
    "        Print(\"subgroup fairness: \", fairness_by_group)\n",
    "        print(\"performance gap: \", KS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
